#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö HTML —Å—Ç—Ä–∞–Ω–∏—Ü —Ñ–æ—Ä—É–º–∞ aum.mybb.ru

–£–¥–∞–ª—è–µ—Ç:
- –í—Å–µ JavaScript —Å–∫—Ä–∏–ø—Ç—ã
- CSS —Å—Ç–∏–ª–∏ —Ñ–æ—Ä—É–º–∞
- –†–µ–∫–ª–∞–º—É (Yandex RTB –∏ –¥—Ä.)
- –ù–∞–≤–∏–≥–∞—Ü–∏—é, —Ñ–æ—Ä–º—ã, –º–µ–Ω—é
- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –ò–º–µ–Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ —Ñ–æ—Ä—É–º–∞
- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≤—Ç–æ—Ä–µ –ø–æ—Å—Ç–∞

–°–æ—Ö—Ä–∞–Ω—è–µ—Ç:
- –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤
- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Å–∫–∞—á–∏–≤–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ)
- –°–ø–æ–π–ª–µ—Ä—ã –∏ —Ü–∏—Ç–∞—Ç—ã (–±–µ–∑ –∏–º—ë–Ω)
- –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
"""

import re
import sys
import hashlib
import urllib.request
import urllib.parse
from pathlib import Path
from html import unescape

try:
    from bs4 import BeautifulSoup, NavigableString
except ImportError:
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ BeautifulSoup: pip install beautifulsoup4")
    sys.exit(1)


# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π CSS –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
MINIMAL_CSS = """
body { font-family: Arial, sans-serif; background: #131833; color: #e0e0e0; line-height: 1.7; padding: 20px; max-width: 900px; margin: 0 auto; }
h1 { color: #99FEFE; text-align: center; }
.post { background: #1a1f3a; border-radius: 10px; padding: 20px; margin: 20px 0; }
.post-content { font-size: 14px; }
.post-content p { margin: 10px 0; }
img.postimg { max-width: 100%; height: auto; border-radius: 5px; }
img[title="float:right"] { float: right; padding-left: 12px; max-width: 40%; }
img[title="float:left"] { float: left; padding-right: 12px; max-width: 40%; }
.quote-box { background: rgba(0,0,0,0.3); border-left: 3px solid #99FEFE; padding: 15px; margin: 15px 0; border-radius: 0 8px 8px 0; }
.spoiler-box > div:first-child { cursor: pointer; color: #99FEFE; font-weight: bold; padding: 10px; background: rgba(0,0,0,0.2); border-radius: 5px; }
.spoiler-box > blockquote { display: none; padding: 15px; }
.spoiler-box.visible > blockquote { display: block; }
.clearer { clear: both; }
a { color: #99FEFE; }
.broken-link { color: #FF6666; text-decoration: underline dotted; cursor: help; }
"""


def fix_image_url(url: str) -> str:
    """
    –ò—Å–ø—Ä–∞–≤–∏—Ç—å –±–∏—Ç—ã–π URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
    - &amp; ‚Üí —É–¥–∞–ª—è–µ—Ç—Å—è (–¥–ª—è Google Drive ?&id= ‚Üí ?id=)
    - –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç http ‚Üí https
    """
    if not url:
        return url
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML entities (&amp; ‚Üí &)
    url = unescape(url)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–π & –ø–æ—Å–ª–µ ? (Google Drive: ?&id= ‚Üí ?id=)
    url = url.replace('?&', '?')
    
    # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –∞–º–ø–µ—Ä—Å–∞–Ω–¥—ã
    url = re.sub(r'&+', '&', url)
    
    # http ‚Üí https
    if url.startswith('http://'):
        url = url.replace('http://', 'https://', 1)
    
    return url


def download_image(url: str, save_dir: Path) -> str | None:
    """
    –°–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ URL –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ø–∞–ø–∫—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ.
    """
    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        parsed = urllib.parse.urlparse(url)
        path_ext = Path(parsed.path).suffix.lower()
        if path_ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']:
            ext = path_ext
        else:
            ext = '.jpg'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        filename = f"img_{url_hash}{ext}"
        save_path = save_dir / filename
        
        # –ï—Å–ª–∏ —É–∂–µ —Å–∫–∞—á–∞–Ω–æ - –Ω–µ –∫–∞—á–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ
        if save_path.exists():
            return filename
        
        # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ—Ç
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–∫–∞—á–∏–≤–∞–µ–º
        headers = {'User-Agent': 'Mozilla/5.0'}
        req = urllib.request.Request(url, headers=headers)
        
        with urllib.request.urlopen(req, timeout=10) as response:
            content = response.read()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            content_type = response.headers.get('Content-Type', '')
            if 'image' not in content_type and len(content) < 100:
                return None
            
            save_path.write_bytes(content)
            return filename
            
    except Exception as e:
        print(f"  ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {url[:50]}...: {e}")
        return None


def process_images(post_content, files_dir: Path, files_dir_name: str) -> int:
    """
    –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–æ—Å—Ç–µ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö.
    """
    downloaded = 0
    
    for img in post_content.find_all('img'):
        # –ü–æ–ª—É—á–∞–µ–º URL (–∏–∑ src –∏–ª–∏ alt)
        src = img.get('src', '')
        alt = img.get('alt', '')
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ URL
        url = None
        for candidate in [src, alt]:
            if candidate and (candidate.startswith('http://') or candidate.startswith('https://')):
                url = fix_image_url(candidate)
                break
        
        # –ï—Å–ª–∏ URL —É–∂–µ –ª–æ–∫–∞–ª—å–Ω—ã–π - –æ—Å—Ç–∞–≤–ª—è–µ–º
        if src and not src.startswith('http'):
            continue
        
        if not url:
            img.decompose()  # –ù–µ—Ç –≤–∞–ª–∏–¥–Ω–æ–≥–æ URL - —É–¥–∞–ª—è–µ–º
            continue
        
        # –ü—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å
        local_file = download_image(url, files_dir)
        
        if local_file:
            # –£—Å–ø–µ—à–Ω–æ - –æ–±–Ω–æ–≤–ª—è–µ–º src
            img['src'] = f"{files_dir_name}/{local_file}"
            img['loading'] = 'lazy'
            img['class'] = 'postimg'
            # –£–±–∏—Ä–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            for attr in ['alt', 'data-src', 'onclick']:
                if attr in img.attrs:
                    del img.attrs[attr]
            downloaded += 1
        else:
            # –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å - —É–¥–∞–ª—è–µ–º —Ç–µ–≥
            img.decompose()
    
    return downloaded


def clean_forum_html(input_path: str, output_path: str = None) -> str:
    """
    –û—á–∏—Å—Ç–∏—Ç—å HTML —Ñ–∞–π–ª —Ñ–æ—Ä—É–º–∞ –æ—Ç –º—É—Å–æ—Ä–∞.
    
    Args:
        input_path: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É HTML —Ñ–∞–π–ª—É
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ None - –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π)
    
    Returns:
        –ü—É—Ç—å –∫ –æ—á–∏—â–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
    """
    input_file = Path(input_path)
    if not input_file.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_path}")
    
    # –ü–∞–ø–∫–∞ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    files_dir_name = input_file.stem + "_files"
    files_dir = input_file.parent / files_dir_name
    
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª (–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏)
    content = None
    for encoding in ['utf-8', 'windows-1251', 'cp1251', 'latin-1']:
        try:
            content = input_file.read_text(encoding=encoding)
            break
        except UnicodeDecodeError:
            continue
    
    if content is None:
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª —Å –∏–∑–≤–µ—Å—Ç–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π")
    
    # –ü–∞—Ä—Å–∏–º HTML
    soup = BeautifulSoup(content, 'html.parser')
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    title_tag = soup.find('title')
    title = title_tag.get_text() if title_tag else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    title = re.sub(r'\s*¬§\w*¬§\s*', '', title).strip()
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø–æ—Å—Ç—ã
    posts = soup.find_all('div', class_='post')
    
    if not posts:
        topic_div = soup.find('div', id=lambda x: x and x.startswith('topic_'))
        if topic_div:
            posts = topic_div.find_all('div', class_='post')
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Å—Ç–æ–≤
    cleaned_posts = []
    total_images = 0
    
    for post in posts:
        post_content = post.find('div', class_='post-content')
        if post_content:
            # 1. –û–ë–†–ê–ë–ê–¢–´–í–ê–ï–ú –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø (—Å–∫–∞—á–∏–≤–∞–µ–º)
            total_images += process_images(post_content, files_dir, files_dir_name)
            
            # 2. –£–î–ê–õ–Ø–ï–ú –ò–ú–ï–ù–ê –ê–í–¢–û–†–û–í –¶–ò–¢–ê–¢
            for cite in post_content.find_all('cite'):
                cite.decompose()
            
            # 3. –£–î–ê–õ–Ø–ï–ú "–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–æ..."
            for p in post_content.find_all('p', class_='lastedit'):
                p.decompose()
            
            # 4. –£–î–ê–õ–Ø–ï–ú "–ü–æ—Å—Ç X –∏–∑ Y" –º–∞—Ä–∫–µ—Ä—ã
            for span in post_content.find_all('span'):
                text = span.get_text()
                if re.match(r'–ü–æ—Å—Ç\s+\d+\s+–∏–∑\s+\d+', text):
                    span.decompose()
            
            # 5. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏
            for a in post_content.find_all('a'):
                href = a.get('href', '')
                # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—Å—ã–ª–∫–∏
                if href.startswith('javascript:') or 'PostBgColor' in href or 'PhrasesBgcolor' in href:
                    a.replace_with(a.get_text())
                # –í–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ—Ä—É–º - –ø–æ–º–µ—á–∞–µ–º –∫–ª–∞—Å—Å–æ–º
                elif 'aum.mybb.ru' in href:
                    a.attrs = {'class': 'broken-link', 'data-original-href': href}
                else:
                    a.attrs = {'href': href} if href and not href.startswith('javascript:') else {}
            
            # 6. –î–æ–±–∞–≤–ª—è–µ–º onclick –¥–ª—è —Å–ø–æ–π–ª–µ—Ä–æ–≤
            for spoiler in post_content.find_all('div', class_='spoiler-box'):
                spoiler['onclick'] = "this.classList.toggle('visible')"
            
            # 7. –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–≥–∏
            for tag in post_content.find_all(['p', 'span', 'div']):
                if not tag.get_text(strip=True) and not tag.find_all():
                    tag.decompose()
            
            cleaned_posts.append(str(post_content))
    
    # –°–æ–±–∏—Ä–∞–µ–º —á–∏—Å—Ç—ã–π HTML
    clean_html = f"""<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
    <style>
{MINIMAL_CSS}
    </style>
</head>
<body>

<h1>{title}</h1>

"""
    
    for i, post_html in enumerate(cleaned_posts, 1):
        clean_html += f"""<!-- –ü–æ—Å—Ç {i} -->
<div class="post">
{post_html}
</div>

"""
    
    clean_html += """</body>
</html>
"""
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    output_file = Path(output_path) if output_path else input_file
    output_file.write_text(clean_html, encoding='utf-8')
    
    if total_images:
        print(f"  üì∑ –°–∫–∞—á–∞–Ω–æ {total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {files_dir_name}/")
    
    return str(output_file)


def process_directory(directory: str, pattern: str = "*.html"):
    """
    –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ HTML —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
    """
    dir_path = Path(directory)
    if not dir_path.is_dir():
        raise NotADirectoryError(f"–ù–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {directory}")
    
    html_files = [f for f in dir_path.glob(pattern) if not f.name.endswith('.bak')]
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(html_files)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    for html_file in html_files:
        try:
            # –°–æ–∑–¥–∞—ë–º backup
            backup = html_file.with_suffix('.html.bak')
            if not backup.exists():
                import shutil
                shutil.copy(html_file, backup)
            
            # –û—á–∏—â–∞–µ–º
            result = clean_forum_html(str(html_file))
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            original_size = backup.stat().st_size
            new_size = Path(result).stat().st_size
            reduction = (1 - new_size / original_size) * 100
            
            print(f"‚úì {html_file.name}: {original_size/1024:.1f}KB ‚Üí {new_size/1024:.1f}KB ({reduction:.0f}% —É–º–µ–Ω—å—à–µ–Ω–∏–µ)")
        
        except Exception as e:
            print(f"‚úó {html_file.name}: {e}")


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("""
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  python clean_forum_html.py <—Ñ–∞–π–ª.html>           # –û—á–∏—Å—Ç–∏—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª
  python clean_forum_html.py <–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è>          # –û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ .html –≤ –ø–∞–ø–∫–µ
  python clean_forum_html.py <—Ñ–∞–π–ª.html> <–≤—ã—Ö–æ–¥>   # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª
        
–ü—Ä–∏–º–µ—Ä—ã:
  python clean_forum_html.py index.html
  python clean_forum_html.py ./pages/
  python clean_forum_html.py raw.html clean.html
""")
        sys.exit(1)
    
    target = sys.argv[1]
    
    if Path(target).is_dir():
        process_directory(target)
    elif Path(target).is_file():
        output = sys.argv[2] if len(sys.argv) > 2 else None
        result = clean_forum_html(target, output)
        print(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {result}")
    else:
        print(f"‚úó –ù–µ –Ω–∞–π–¥–µ–Ω–æ: {target}")
        sys.exit(1)
